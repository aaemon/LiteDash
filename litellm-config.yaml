model_list:
  # ==========================================
  # üèÜ PREMIUM & REASONING TIER
  # ==========================================
  - model_name: gpt-oss-120b
    litellm_params:
      model: ollama/gpt-oss:120b
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000008  # $0.80 per 1M
      output_cost_per_token: 0.0000020 # $2.00 per 1M

  - model_name: deepseek-r1
    litellm_params:
      model: ollama/deepseek-r1:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000006  # $0.60 per 1M
      output_cost_per_token: 0.0000015 # $1.50 per 1M

  # ==========================================
  # üöÄ HIGH-PERFORMANCE TIER (70B - 80B)
  # ==========================================
  - model_name: qwen3-next-80b
    litellm_params:
      model: ollama/qwen3-next:80b
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0.0000005  # $0.50 per 1M
      output_cost_per_token: 0.0000015 # $1.50 per 1M

  - model_name: qwen3-next
    litellm_params:
      model: ollama/qwen3-next:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.0000015

  - model_name: nemotron
    litellm_params:
      model: ollama/nemotron:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.0000015

  - model_name: gpt-oss
    litellm_params:
      model: ollama/gpt-oss:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.0000005
      output_cost_per_token: 0.0000015

  # ==========================================
  # üõ†Ô∏è SPECIALIST TIER (Coding, Vision, Tasks)
  # ==========================================
  - model_name: qwen3-coder-next
    litellm_params:
      model: ollama/qwen3-coder-next:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 131072
      input_cost_per_token: 0.0000002  # $0.20 per 1M
      output_cost_per_token: 0.0000006 # $0.60 per 1M

  - model_name: qwen3-vl
    litellm_params:
      model: ollama/qwen3-vl:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0.00000015 # $0.15 per 1M
      output_cost_per_token: 0.0000004 # $0.40 per 1M

  - model_name: translategemma
    litellm_params:
      model: ollama/translategemma:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 8192
      input_cost_per_token: 0.0000001
      output_cost_per_token: 0.0000002

  - model_name: deepseek-ocr
    litellm_params:
      model: ollama/deepseek-ocr:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0.00000005
      output_cost_per_token: 0.0000001

  - model_name: glm-ocr
    litellm_params:
      model: ollama/glm-ocr:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0.00000005
      output_cost_per_token: 0.0000001

  - model_name: gpt-oss-safeguard
    litellm_params:
      model: ollama/gpt-oss-safeguard:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 8192
      input_cost_per_token: 0.0000001
      output_cost_per_token: 0.0000002

  # ==========================================
  # ‚ö° ECONOMY TIER (Fast & Cheap)
  # ==========================================
  - model_name: qwen3
    litellm_params:
      model: ollama/qwen3:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 32768
      input_cost_per_token: 0.00000005 # $0.05 per 1M
      output_cost_per_token: 0.0000001 # $0.10 per 1M

  - model_name: glm-4.7-flash
    litellm_params:
      model: ollama/glm-4.7-flash:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 128000
      input_cost_per_token: 0.00000005
      output_cost_per_token: 0.0000001

  # ==========================================
  # üß† EMBEDDINGS TIER (For RAG systems)
  # ==========================================
  - model_name: nomic-embed-text
    litellm_params:
      model: ollama/nomic-embed-text:latest
      api_base: "http://host.docker.internal:11434"
    model_info:
      max_tokens: 8192
      input_cost_per_token: 0.00000001 # $0.01 per 1M
      output_cost_per_token: 0.0       # Usually 0 for embeddings

# ==========================================
# SYSTEM SETTINGS
# ==========================================

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"

litellm_settings:
  drop_params: True
  set_verbose: False
